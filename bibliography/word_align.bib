@article{brown-etal-1993-mathematics,
    title = "The Mathematics of Statistical Machine Translation: Parameter Estimation",
    author = "Brown, Peter F.  and
      Della Pietra, Stephen A.  and
      Della Pietra, Vincent J.  and
      Mercer, Robert L.",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J93-2003",
    pages = "263--311",
}

@article{Ostling2016efmaral,
  title = {Efficient word alignment with {M}arkov {C}hain {M}onte {C}arlo},
  author = {Robert {\"O}stling and J{\"o}rg Tiedemann},
  journal = {Prague Bulletin of Mathematical Linguistics},
  year = {2016},
  month = {10},
  pages = {125--146},
  volume = {106},
  owner = {robert},
  timestamp = {2016.08.26},
  url = {http://ufal.mff.cuni.cz/pbml/106/art-ostling-tiedemann.pdf}
}

@inproceedings{och-ney-2000-improved,
    title = "Improved Statistical Alignment Models",
    author = "Och, Franz Josef  and
      Ney, Hermann",
    booktitle = "Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics",
    month = oct,
    year = "2000",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P00-1056",
    doi = "10.3115/1075218.1075274",
    pages = "440--447",
}


@inproceedings{och-ney-2003-smt,
author = {Koehn, Philipp and Och, Franz Josef and Marcu, Daniel},
title = {Statistical Phrase-Based Translation},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073445.1073462},
doi = {10.3115/1073445.1073462},
abstract = {We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.},
booktitle = {Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1},
pages = {48â€“54},
numpages = {7},
location = {Edmonton, Canada},
series = {NAACL '03}
}


@article{och-ney-2003-systematic,
    title = "A Systematic Comparison of Various Statistical Alignment Models",
    author = "Och, Franz Josef  and
      Ney, Hermann",
    journal = "Computational Linguistics",
    volume = "29",
    number = "1",
    year = "2003",
    url = "https://aclanthology.org/J03-1002",
    doi = "10.1162/089120103321337421",
    pages = "19--51",
}


@book{koehn-2020,
  place={Cambridge}, 
  title={Neural Machine Translation}, 
  DOI={10.1017/9781108608480}, 
  publisher={Cambridge University Press}, 
  author={Koehn, Philipp}, 
  year={2020}
}


@inproceedings{artetxe-etal-2018-robust,
    title = "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
    author = "Artetxe, Mikel  and
      Labaka, Gorka  and
      Agirre, Eneko",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1073",
    doi = "10.18653/v1/P18-1073",
    pages = "789--798",
    abstract = "Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at \url{https://github.com/artetxem/vecmap}.",
}


@misc{delvin-chang-2018-bert,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{conneau-etal-2020-xlm,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}

@inproceedings{dyer-etal-2013-simple,
    title = "A Simple, Fast, and Effective Reparameterization of {IBM} Model 2",
    author = "Dyer, Chris  and
      Chahuneau, Victor  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N13-1073",
    pages = "644--648",
}

@manual{koehn-moses-smt-2022,
  title = {Moses. Statistical Machine Translation System. User Manual and Code Guide},
  author = {Philipp Koehn},
  month = {4},
  year = {2022},
  url = {http://www2.statmt.org/moses/manual/manual.pdf}
}