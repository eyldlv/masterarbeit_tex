\chapter{Data Collection}
\section{Introduction}

The corpus at hand incorporates the press releases published by the Canton of Grisons/Graubünden. 
These press releases are a means of the cantonal government to publish news and information about topics such as politics, economy, health and culture. 
Graubünden, which is made up of German speaking, Italian speaking and Romansh speaking regions, is the only trilingual canton in Switzerland. 
As such, virtually all press releases are published in German, Italian and Romansh. 
This trilingual setting lends itself to be collected to a parallel trilingual corpus.

\section{Data Collection}
At first, I contacted the \emph{Standeskanzlei} (\enquote{State Chancellery of Grisons}) which is the \enquote{the general administrative authority for questions of office, coordination and liaison with the cantonal parliament (\enquote{Grosser Rat}), government and cantonal administration} \autocite{staka}. 
The \emph{Standeskanzlei}, with its \emph{Übersetzungsdienst} (\enquote{Translation service}), is responsible for translating documents in service of the canton.
I was hoping to receive the data directly from them -- after all, we are not talking about private or commercial data, but about public translation work financed with taxpayers' money.
I spoke to Mr. Mirco Frepp from the communication sevices (\emph{Kommunaktionsdienst}), which, although very friendly, had to inform me that it would be impossible for me to receive the data. 
The explanation was that the documents are not saved locally somewhere, but are saved in a database. 
The documents are extracted from the database and are generated as ad-hoc HTML documents whenever the website is accessed. 
It was also not possible to receive a dump of the database.

\subsection{Web scraping}
Not being able to receive a dump of the database meant I had to scrape the canton's website, extract the relevant content from the HTML files and construct my own database. In order to achieve this, I wrote a series of Pyhton scripts that would take care of these tasks. 
All the scripts can be found on my GitHub/Gitlab repository. 
The scripts relevant for the database building are saved under the folder \texttt{corpus\_builder}

\begin{figure}
\dirtree{%
.1 corpus\_builder.
.2 corpus\_builder.
.3 access\_db.py.
.3 create\_corpus.py.
.2 web\_scraper.
.3 test\_web\_scraper.py.
.3 web\_scraper.py.
}

\caption{Directory tree of \texttt{corpus\_builder}}
\end{figure} 
\subsubsection{\texttt{web\_scraper.py}}
The script goes to the main page for each year and language. 
That page contains a links pointing to all the press releases that were released that year. 
It collects all those links and then downloads the HTML from each link. 
The HTML pages are saved to seperate folders for each year. 
The filenames have the following format: \texttt{year\_file-id\_language}, e.g., \texttt{1997\_12924\_DE.html}. 
The file-id is taken from the URL and will be later used to align the documents. 


\begin{figure}
\dirtree{%
.1 html.
.2 1997.
.3 1997\_12924\_DE.html.
.3 1997\_12936\_IT.html.
.3 {...}.
.2 1998.
.2 1999.
.2 {...}.
.2 2022.
.3 2022\_2022010301\_DE.html.
.3 2022\_2022010301\_IT.html.
.3 2022\_2022010301\_RM.html.
.3 {...}.
}
\caption{Directory scheme for saving the HTML files}
\label{fig:html-scheme}
\end{figure} 

Since the script makes many requests to the website, one has to expect that server might stop responding, which will result in a request time-out. 
To avoid downloading HTML pages that were already downloaded, the script will skip and press realease that already exists locally, providing the file size is greater than 0 bytes.
This way, the script can be run at a later stage, after new press releases were published, to complete the local repository.
To make sure the local copy of the press releases is complete, the script can be simply run until a message is printed that no new press releases were downloaded. 

By deafult, the script will download the press releases for the entire year range (1997 to the current year) and in all three languages. 
This can be limited by using the following optional arguments:
\begin{itemize}
	\item \texttt{-{}-year} -- limit the scraping to a year or to a range of years seperated by a comma, e.g., \texttt{-{}-year 2022} or \texttt{-{}-year 2020,2022}
	\item \texttt{-{}-lang} -- limit the scraping to one or more languages (comma seperated), e.g., \texttt{-{}-lang de,it}
\end{itemize}

\subsection{Content extraction}


\subsection{Document Alignment}
The next stage after saving all of the press releases locally is to align them. 
For all releases published after mid 2002 this is pretty simple. 
The file-id extracted from the URLs is common to all three releases in the three languages (see figure~\ref{fig:html-scheme}). 
This file-id can be used to link the press releases with each other.

For releases published prior to that, each release carries a unique file-id. 
This means it can't be used for document alignment. 
Here, I used a simple heuristic: if on one date three releases were published in three different languages, I assume they are translations of each other. 
The titles of press releases that weren't aligned are saved to a CSV file which can be used for manual alignment.


