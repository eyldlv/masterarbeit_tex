\chapter{Compiling the Corpus}
\section{Introduction}

The corpus at hand incorporates the press releases published by the Canton of Grisons/Graubünden. 
These press releases are a means of the cantonal government to publish news and information about topics such as politics, economy, health and culture. 
Graubünden, which is made up of German speaking, Italian speaking and Romansh speaking regions, is the only trilingual canton in Switzerland. 
As such, virtually all press releases are published in German, Italian and Romansh. 
This trilingual setting lends itself to be collected to a parallel trilingual corpus.

\section{Collecting the Data}
At first, I contacted the \emph{Standeskanzlei} (\enquote{State Chancellery of Grisons}) which is the \enquote{the general administrative authority for questions of office, coordination and liaison with the cantonal parliament (\enquote{Grosser Rat}), government and cantonal administration} \autocite{staka}. 
The \emph{Standeskanzlei}, with its \emph{Übersetzungsdienst} (\enquote{Translation service}), is responsible for translating documents in service of the canton.
I was hoping to receive the data directly from them -- after all, we are not talking about private or commercial data, but about public translation work financed with taxpayers' money.

I spoke to Mr.~Mirco Frepp from the communication sevices (\emph{Kommunaktionsdienst}), which, although very friendly, had to inform me that it would be impossible for me to receive the data. 
The explanation was that the documents are not saved locally somewhere, but are saved in a database. 
The documents are extracted from the database and are generated as ad-hoc HTML documents whenever the website is accessed. 
It was also not possible to receive a dump of the database.

\section{Web Scraping}
Not being able to receive a dump of the database meant I had to scrape the canton's website, extract the relevant content from the HTML files and construct my own database. In order to achieve this, I wrote a series of Pyhton scripts that would take care of these tasks. 
All the scripts can be found on my GitHub/Gitlab repository. 
The scripts relevant for the database building are saved under the folder \texttt{corpus\_builder}.

\begin{figure}
\dirtree{%
.1 corpus\_builder.
.2 corpus\_builder.
.3 access\_db.py.
.3 create\_corpus.py.
.2 web\_scraper.
.3 test\_web\_scraper.py.
.3 web\_scraper.py.
}

\caption{Directory tree of \texttt{corpus\_builder}}
\end{figure} 
\subsubsection{Web Scraper}
The script \texttt{web\_scraper.py} goes to the index web page for each year and language. 
This page contains a links pointing to all the press releases that were released that year. 
It collects all those links and then downloads the HTML from each link. 
The HTML pages are saved to seperate folders for each year. 
The filenames have the following format: \texttt{year\_file-id\_language}, e.g., \texttt{1997\_12924\_DE.html}. 
The file-id is taken from the URL and will be later used to align the documents. 


\begin{figure}
\dirtree{%
.1 html.
.2 1997.
.3 1997\_12924\_DE.html.
.3 1997\_12936\_IT.html.
.3 {...}.
.2 1998.
.2 1999.
.2 {...}.
.2 2022.
.3 2022\_2022010301\_DE.html.
.3 2022\_2022010301\_IT.html.
.3 2022\_2022010301\_RM.html.
.3 {...}.
}
\caption{Directory scheme for saving the HTML files}
\label{fig:html-scheme}
\end{figure} 

Since the script makes many requests to the website, one has to expect that server might stop responding, which will result in a request time-out. 
To avoid downloading HTML pages that were already downloaded, the script will skip and press realease that already exists locally, providing the file size is greater than 0 bytes.
This way, the script can be run at a later stage, after new press releases were published, to complete the local repository.
To make sure the local copy of the press releases is complete, the script can be simply run until a message is printed that no new press releases were downloaded. 

By deafult, the script will download the press releases for the entire year range (1997 to the current year) and in all three languages. 
This can be limited by using the following optional arguments:
\begin{itemize}
	\item \texttt{-{}-year} -- limit the scraping to a year or to a range of years seperated by a comma, e.g., \texttt{-{}-year 2022} or \texttt{-{}-year 2020,2022}
	\item \texttt{-{}-lang} -- limit the scraping to one or more languages (comma seperated), e.g., \texttt{-{}-lang de,it}
\end{itemize}

\section{Building the Corpus}
All the scripts responsible for buidling the corpus can be found under the folder \texttt{corpus\_builder}.

\subsection{HTML Parsing}
After the creation of a local copy of the HTML files containing the press releases, the content needs to be extracted from the HTML files and saved in a format that would be suitable for later processing.

Using the Python package BeautifulSoup to parse the HTML files, I extracted from each HTML file the title and the text of the press release, as well as some meta data: date, language and the original file-id and the original file name (for debugging purposes).  
The data was then saved to a JSON file, one per each year.
See listing~\ref{lst:json} for an example.


\begin{lstlisting}[caption={Example for a JSON file}, captionpos=b, label=lst:json, language=json]
{
    "0": {
        "id": "12924",
        "orig_file": "html/1997/1997_12924_DE.html",
        "lang": "DE",
        "title": "25 Jahre Arge Alp: Graubünden feiert tüchtig mit",
        "date": "31.12.1997",
        "content": "Die Arge Alp feiert heuer ihr 25-Jahre-Jubiläum. Aus diesem Grund finden vom 27. September bis 12. Oktober 1997 die Festwochen des Alpenraums in Telfs-Mösern, Tirol, statt. ..."
    },
    "1": {
        "id": "12926",
        "orig_file": "html/1997/1997_12926_DE.html",
        "lang": "DE",
        "title": "Kanton will auch personelles Engagement bei der Bündner Kraftwerke AG verstärken",
        "date": "31.12.1997",
        "content": "Nachdem der Kanton Graubünden letzten Herbst die Aktienmehrheit der Bündner Kraftwerke AG übernommen hat, will er nun auch seine Vertretung im Verwaltungsrat stärken...."
    },
    "2": {
        "id": "12927",
        "orig_file": "html/1997/1997_12927_DE.html",
        "lang": "DE",
        "title": "Graubünden trifft präventive Massnahmen zur Bekämpfung der illegalen Einwanderung",
        "date": "31.12.1997",
        "content": "Die Fremdenpolizei des Kantons Graubünden trifft im Einvernehmen mit dem kantonalen Sozialamt, dem Amt für Zivilschutz sowie der Kantonspolizei Graubünden Massnahmen, um die illegale Einwanderung in den Südtälern des Kantons Graubünden zu bekämpfen...."
    },
\end{lstlisting}

\subsection{Document Alignment}
After extracting the relevant data from the HTML files and saving them in JSON files, the core task can begin: aligning the documents to get document-triples which are translations of each other.

\subsubsection{Linked vs.~unlinked}
\label{sec:linked-unlinked}
For all releases published after mid-2009 this is pretty simple. 
The file-id extracted from the URLs is common to all three releases in the three languages (see figure~\ref{fig:html-scheme}). 
This file-id can be used to link the press releases with each other. 
I shall refer to these press releases as \enquote{linked releases}.

For releases published prior to that, each release has a unique file-id. 
This means it can't be used for document alignment. 
I shall refer to these releases as \enquote{unlinked releases}.
For unlinked releases I used a simple heuristic: if on one single date exactly three releases were published in three different languages, I assume they are translations of each other. 
The titles of press releases that weren't aligned are saved to a CSV file which can be used for manual alignment.

Unfortunately, this means around 40\% (TODO: what is the exact average?) of the releases each year prior to 2009 cannot be automatically added to the corpus, cf.~figure~\ref{fig:linked-unlinked}.

\begin{figure}[h]
	\centering
	\includegraphics{graphics/linked-unlinked.png}
	\label{fig:linked-unlinked}
	\caption{Portion of automatically aligned press releases up to 2009}
\end{figure}

Since the year 2009 contains both linked and unlinked releases, I wrote the script \texttt{split\_2009.py} to split the data accordingly. 
It uses a very simple heuristic: if the file-id of a press release is longer than 5 digits, it is a linked press releases.

\subsubsection{Aligned corpus}
The aligned press releases are saved again to JSON files, with each row containing the three press releases in the three languages, along with metadata such as date and file-id. 
In the rare case that one language is missing (TODO: how rare?), i.e., the press releases wasn't translated into that language for some reason, it is simply left blank. 
Press releases that are available only in one language are discarded from the corpus.

The script \texttt{create\_corpus.py} deals with this task.
Using the Python library Pandas, the JSON files are read into a Dataframe. 
For linked releases, all the unique ID's are taken, and then for each ID the three languages are collected and saved into a new row. 
The dates are converted from their original format (DD.MM.YY) to an ISO 8601 format (YYYY-MM-DD) \autocite{enwiki:1095673391} for better compatibility and easier processing later.

For JSON files contatining unlinked documents, the script \texttt{create\_corpus} has to be run with the switch \texttt{-{}-by-date}, which tells the program to use the date for aligning the documents instead of the file ID.

For an example of the resulting JSON files, with each row containing the aligned documents, see listing~\ref{lst:json-aligned}.
\input{listings/aligned_json}

\section{Manual alignment of unlinked documents}
As can be seen in figure~\ref{fig:linked-unlinked}, a big portion of the unlinked documents cannot be automatically aligned using the simple heuristic described in section~\ref{sec:linked-unlinked}. 
To deal with that, the script \texttt{create\_corpus.py} will write the titles and file IDs of all the discarded documents to a CSV file, one for each year. 

These CSV files can be used for manually aligning the corresponding documents. 
This is done by enumerating the documents while using the same digit for corresponding documents. 
The CSV files containing the now enumerated documents can be given to the script \texttt{create\_corpus.py} using the argument \texttt{add-from-csv} to combine the enumerated documents as linked documents into the JSON file.

\section{SQLite database}
The query language SQL offers flexible and complex way to query datbases. 
For this reason, I decided to save the resulting corpus in an SQLite database. 
I opted for SQLite because it doesn't require running a seperate server and can be SQLite databases can be easily built, edited and accessed using \texttt{sqlite3}\footnote{\url{https://docs.python.org/3/library/sqlite3.html}}, a Python module delivered in the Python standard library\footnote{\url{https://docs.python.org/3/tutorial/stdlib.html}}.

The SQLite database contains two tables, \texttt{corpus} and \texttt{raw} with the exact same structure as the two JSON files described in listings~\ref{lst:json} and \ref{lst:json-aligned}.

% \footnotetext{I opted for SQLite instead of mySQL since SQLite does not require installing any special software or running a server locally. The Python library sqlite3 is shipped with Python as a standard package.}

\section{Summary}
For compiling the corpus, the following steps were taken:
\begin{enumerate}
	\item Scrape website and save HTML documents locally
	\item Extract relevant content from HTML files (date, language, title and content) and save to JSON files
	\item Read the JSON files using Pandas Dataframe, align the documents and save to new JSON files
	\item Feed both types of JSON files to an sqlite database
\end{enumerate}

The final result is an sqlite database (\texttt{corpus.db}) containing two tables:
\begin{itemize}
	\item \texttt{corpus}: all the aligned documents from 1997 until today. 
	Each row contains following columns:
	\begin{itemize}
		\item id: automatically incremented unique ID
		\item file\_id: original file ID
		\item date: Release date
		\item DE\_title: Title of German document
		\item DE\_content: Content of German document
		\item IT\_title: Title of Italian document
		\item IT\_content: Content of Italian document
		\item RM\_title: Title of Romansh document
		\item RM\_content: Content of Romansh document
	\end{itemize}
	\item \texttt{raw}: all the documents contained in the HTML files scraped from the website. 
	Each row contains the following columns:
	\begin{itemize}
		\item id: Automatically incremented unique ID
		\item file\_id: Original file ID
		\item orig\_file: Original filename
		\item lang: Document language (DE for German, IT for Italian, RM for Romansh)
		\item title: Document title
		\item date: Release date
		\item content: Document content
	\end{itemize}
\end{itemize}



