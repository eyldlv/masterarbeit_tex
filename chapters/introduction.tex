\chapter{Introduction}
\section{Motivation}
The Romansh language is a Romance language spoken in Switzerland, primarily in the Canton of Grisons (henceforth \emph{Graubünden}), by around 60,000 speakers. 
Graubünden is the only canton in Switzerland with three official languages -- German, Italian and Romansh.

When traveling by train, the announcements are heard in German, Romansh or Italian according to which part of the canton one is currently at. 
It is enough to travel to the next valley to suddenly be greeted on the street in a different language.
Newspapers, radio and television exist in all three languages, but also official documents, laws and press releases are published trinlingually.
While I was resident in Graubünden, I was fascinated by this multilinguality and it was my wish to somehow capture it and make it available to others. 
This is why I decided to build a multilingual corpus, a parallel collection of  sentences in German, Romansh and Italian, in which the sentences are translations of each other.

Having such a low number of speakers turns it into a so-called low resource language. 
Having so little speakers means there is also little data, be it corpora or research data.
Most of the reasearch in NLP focuses on high resource languages. 


\section{Research Question and Goals}
\cite{jalili-sabet-etal-2020-simalign} were able to show that their algorithm for word alignment outperforms all the statistical baseline models. 
Contrary to statistical models, their model uses vectors of word representations learned by a neural net (also commonly known as word embeddings) and, by using some sort of similarity measurement (e.g., cosine similarity), aligns the most similar words in the source and the target sentence. 

But not only that the model outperforms the existing stastical models, its biggest advantage as propogated by \cite{jalili-sabet-etal-2020-simalign} is that it requires no training data. 
Statistical models will only reach a threshold of good performance with enough training data (TODO: cite numbers from SimAlign). 
Using word embeddings can be used to align words in just a single sentence with high precision. 
Of course, all of this works persuming we already have a trained model whose learned embeddings we can use for this task. 
There exist some language models that were trained on multi-lingual data. 
mBERT was trained on 104 languages and LASER was trained on 93 languages. 
But will word embeddings based word alignment will work in zero-shot settings? 
That is, can the embeddings learned by a multilingual language model be used for word alignment for a language that wasn't included in the training data?

