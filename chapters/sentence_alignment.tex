\chapter{Sentence Alignment}

\section{Introduction}

The corpus presented in chapter~\ref{chap:compiling} is a raw parallel corpus, that is, it is a corpus of aligned documents without any further processing. 
In order to use the corpus for tasks such as training a machine translation model, another processing step is needed: sentence alignment \autocite[55]{koehn2009}.

A bilingual, sentence-aligned corpus can be useful for a variety of tasks. 
Probably the most important task bilngual corpora are used for nowadays is for training a machine translation model \autocites{gale-church-1991-program,moore2002fast,chen-1993-aligning}, but other tasks it can be used for are building translation memories \autocite{sennrich-volk-2011-iterative} or a for a bilingual concordance system with the purpose of allowing a user to find out how a given translation is translated \autocites{moore2002fast,gale-church-1991-program}.

Formally, the task can be described as follows: We have a list of sentences in language \(e\), \(e_1,...e_{n_e}\) and a list of sentences in language \(f\), \(f_1,...,f_{n_f}\). 
(Note that the number of sentences in each language is not necessarily identical.) 
A sentence alignment \(S\) consists of a list of sentence pairs \(s_1, ..., s_n\), such that each sentence pair \(s_i\) is a pair of sets:

\[
	s_i = ( \{ e_{\text{start-e}(i)},... , e_{\text{end-e}(i)}\}, \{f_{\text{start-f}(i)},... , f_{\text{end-f}(i)}\} )
\]
\autocite[56]{koehn2009}

This means each set in the pair of set can consist of one or more sentences. 
The numer of sentences in each set is referred to as alignment type. 
A 1--1 alignment is an alignment where exactly one sentence of language \(e\)
is aligned to exactly one sentence of language \(f\). 
In a 1--2 alignemnt, one sentence in lanauge \(e\) is a aligned to two sentences in langauge \(f\). 
There are also 0--1 alignments, in which a sentence of language \(f\) is not aligned to anything of language \(e\). 
Sentences may not be left out and each sentence may only occur in one sentence pair \autocite[57]{koehn2009}. 


\section{Overview}
\label{sec:overview_senalign}
Traditionally, there are three main approaches for solving the problem of sentence alignment:  length-based, dictionary- or translation-based and partial similarity-based \autocite{hunalign}. 

\subsection{Length Based}
One early method for sentence alignment is the one described in \cite{gale-church-1991-program} which is \enquote{based on a simple satistical model of character lengths} \autocite{gale-church-1991-program}. The method arose out of the need to design a faster, computationally more efficient algorithm\footnotemark.

\footnotetext{With the algorithms that existed up to that time, it took 10 days to extract 3 million sentence pairs, 12,500 sentences per hour.}

The method uses the fact that longer sentences in language \(e\) are usually translated into longer sentences in language \(f\) and vice-versa -- shorter sentences correspond to shorter sentences.

The method combines a distance measure based on the lengths of the sentence with a prior probility of the alignment type (1--1, 1--0 or 0--1, 2--1 or 1--2, 2--2) to a probabilistic score. 
It assigns this score to possible sentence pairs in a dynamic programming framework to find the best (most probable) pairs. 
A program based on this method was tested against a human-made alignment on two pairs of languages: English-German and English-French. 
The program made a total of 55 errors out of a total of 1316 alignments (4.2\%). 
By taking the best scoring 80\% of the alignments, the error rate could be reduced to 0.7\%

The method was also much faster than the algorithms that existed up to that time. 
It took 20 hours to extract around 890,000 sentence pairs, around 44,500 sentence pairs per hour, around 3.5 times faster than former algorithms.

\subsection{Partial Similarity Based}
Another method is similarity based such as the one presented in \cite{simard-plamondon-1996-bilingual}. 
Here, alignment follows two steps. 
In the first step, isolated cognates are used to mark sort of \emph{anchors} in the texts. 
The term cognate refers here to two word-forms of different language whose four first characters are identical. 
Isolated cognates are cognates with no resmebling word forms within a context window.
It follows the assumption that two isolated cognates of different languages are parts of segments that are mutual translations and should be aligned with each other. 
These cognates are used as anchors and the process is repeated recursively between the anchors until no more anchor points can be found.

In an intermediate step, segmentation into sentence boundaries takes place and the search space is determined, i.e., it is determined, based on the anchors found in the first step, which sentences could be aligned with each other. 
Only sentence-pairs that are within the same search space boundaries are alignment candidates.

In the second step, the final alignment takes place. 
Theoretically, any sentence alignment program that can operate within the restricted search space defined in the previous steps can take over the job. 
In \cite{simard-plamondon-1996-bilingual}, the authors use a statistical lexical translation mdoel (commonly known as IBM Model 1), to measure how probable it is to observe one sentence given another sentence and so find the sentences that are most probably mutual translations.

\subsection{Translation based}

Another possibility for aligning sentences is translation based. 
Here, the alignment algorithm constructs a statistical word-to-word translation model of the corpus. 
It then finds the sentence alignment that maximizes the probability of of generating the corpus with this translation model. 
In other words, it aligns sentences that are most likely translations of each other, given the translation model \autocite{chen-1993-aligning}. 

\subsection{Hybrid models}
There are also hybrid sentence-alignment methods, combining several methods.


\cite{moore2002fast} presents a method in which sentence lengths are combined with word correspondences to find the best alignments. 
It works in three steps:
First sentences are aligned using a sentence-length-based model. 
Then, the sentence pairs with the highest probabiltiy, i.e., those that are most likely real correspondences of each other, are taken to train a translation model. 
The translation model is then used to augment the initial alignment, so that the result is length- and translation-based.

Another hybrid method was presented in \cite{hunalign}. 
It combines a dictionary- and a length-based method.
Here a sort of a dummy translation of the source text is produced using a translation dictionary by simply converting each token into its corresponding dictinoary translation. 
Then, for each sentence pair a similarity score is computed. 
It consists of two components: a score based the number of shared words in the sentnce pair (token based);  a score based on the ratio of character counts between sentences. 
The program treats paragraph boundaries as sentences with special scoring. 
The similarity score of a paragraph-boundary  and a real sentence is always minus infinity, which makes sure they never align. This way, paragraph boundaries always align with themselves and can be used as anchors to make sure paragraphs remain aligned.

\subsection{Summary}
Stichpunkte:
\begin{itemize}
	\item All models perform well.
	\item Already the model described in \cite{gale-church-1991-program} achieved a precision of 95.8\%.
	\item The motivaiton was rather design a faster model rather than a better performing model \autocite{chen-1993-aligning}.
\end{itemize}


\section{More Recent methods}
While the statisics based methods described in section~\ref{sec:overview_senalign} date back to the 1990's, more recently different methods were suggested.

\subsection{Bleualign}

One of these methods was presented in \cite{sennrich-volk-2010-mt} and has been cited since as Bleualign. 
It rose as a method addressing to the problem of aligning less \enquote{easily} alignable corpora. 
Sentence alignment methods up to that time perform excellent on well-strctured corpora with a high language similarity such as the Canadian Hansards (transcriptions of parliamentary debates which exist in English and in French) which are considered easy to align  or the Europarl (parliamentary proceedings of the EU Parliament) because they are well-structured, they provide markup information to identify speakers which is useful for creating anchor points and the subsequent alignment \autocites{simard-plamondon-1996-bilingual,sennrich-volk-2011-iterative}. 
However, when aligning pairs of languages which are fundementally different and/or of less structured texts, the alignment task becomes more difficult \autocite{sennrich-volk-2010-mt}.

Bleualign uses BLEU as a similarity score to find sentences alignments. 
BLEU, which satands for Bilingual Evaluation Understudy, is a popular automatic metric for evaluating machine translation models. It measures the similarity between two sentences by considering matches of several n-grams (sequences of tokens of length \(n\); usually scores are combined for n-grams of order 1--4). 
The higher the BLEU score, the higher the similarity between two sentences \autocite[226]{koehn2009}.

Although BLEU has been criticized as a measure of translation quality, BLEU scores can be used for deciding weather two sentences are mutual translations---BLEU scores for two unrelated sentences is usually 0 \autocite{sennrich-volk-2010-mt}. 

Instead of aligning sentences of the source and the target language with each other, Bleualign aligns a machine translated version of the target side of the corpus with the source side to find the most reliable alignments. However, this approach requires an already existing machine translation system with reasonable performance. 
This problem was addressed in \cite{sennrich-volk-2011-iterative} by suggesting an iterative method for alignment combining length-based and BLEU score-based methods which doesn't require an already existing machine translation system. 
In the first iteration, sentences are aligned using an implementation of the Gale \& Church algorithm, then an SMT (statistical machine translation) system is trained on the sentence-aligned corpus. 
In the following iterations, the corpus (target side) is machine translated using the SMT system trained in the last iteration and is then aligned to the source side using Bleualign. 
Then, a new SMT system is trained using the current alignments.

\cite{sennrich-volk-2011-iterative} do not recommend this iterative sentence alignment using Bleualign for all purposes. 
It should be used mainly where conventional sentence alignment algorithms such as Gale \& Church have lower accuracy or where language-specific resources such as dictionaries (used for hunalign \autocite{hunalign}) or machine translation systems are unavailable or lacking in quality.

\subsection{Vecalign}
The wish for  sentence alignment of even higher quality rose with the insight that while misaligned sentences have small effect on SMT performance, they have a crucial effect on neural MT (NMT) systems. 
This is especially true in scenarios with less data for low-resource MT \autocite{thompson-koehn-2019-vecalign}. .

Vecalign uses a novel method which is based on the similarity of bilingual sentence embeddings. 
Sentence embeddings are, in a manner similar to word embeddings (cf. section TODO), numerical representation of sentences that are learned by and can be extracted from a neural language model. 
This numerical vector representation is said to represent the meaning of a sentence. 
If two sentences are similar, they will lie close to each other in the vector space. 
A function that is most often used for measuring vector similarity is the cosine similarity.


DISADVANTAGE: LASER model has to contain the language.

\section{Sentence alignment pipeline}

I shall now describe the pipeline I used for retrieving sentence pairs for the corpus I compiled in section~\ref{chap:compiling}.

My tool of choice was \texttt{hunalign} \autocite{hunalign}. 
It is presented as a software package on GitHub, it is free to use (and contrary to the Microsoft TODO, corpora produced by it are also free to distribute), is well documented, was easy to compile on my system (MacBook Air M1, 2020 running MacOS Monterey 12.3.1) and runs fast. 

I tried, just for the sake of interst, to use \texttt{Vecalign} on a small portion of my corpus. 
Veclaign requires that all adjacent sentences be concetanted first (to consider 1-to-many alignments). 
Then for each sentence-concetanation the sentence embeddings have to be obtained from the LASER language model. 
Only then sentence alignment can be calculated. 
The process of obtaining the sentence alignment took quite some time---around 10 minutes for 300 sentences---and by quick inspection by eye the result wasn't better than that gained with \texttt{hunalign}, but rather worse. 
Obviously, this may be due to the fact that Romansh is not one of the languages LASER was trained on. 
However, LASER has been observed to work well also in zero-shot scenarios with unseen languages that are similar to the ones the model was trained on.

