\chapter{Results}

After having created a gold standard (see Chapter~\ref{chap:gold-standard}) for evaluating the quality of the alignments, I compared the alignments computed by SimAlign with the alignments computed by a baseline system.
I shall now proceed to present the results of the experiment.

\section{Evaluation Metrics}
To evaluate the quality of word alignment, four measures are used. 
The first three---percision, recall and F-mesaure---are traditional measures in information retrieval \autocite{mihalcea-pedersen-2003-evaluation}.

Precision is the percent of true positives out of the items marked by the system as positive. 
It answers the question, how many of the items marked as positive are true positives, and is normally defined as $\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}}$, where TP referes to true positives and FP to false positives.

Recall is the percent of true positives out of all positives retrieved. 
It answers the question, how many of all the true positives were found by the system. 
It is normally defined as $\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}$, where TP refers to true positives and FN to false negatives.

F-measure is a score averaging precision and recall. 
The fourth measurement, \acrfull{aer}, was introduced by \cite{och-ney-2000-improved}. 

For the task of evaluating the word alignment, I used a script made available on GitHub\footnotemark by the creators of SimAlign \autocite{jalili-sabet-etal-2020-simalign}. 

\footnotetext{\url{https://github.com/cisnlp/simalign/blob/master/scripts/calc_align_score.py}}

The script uses a definition of precision, recall and AER which stems from \cite{och-ney-2000-improved} and was later used by many others \autocites{mihalcea-pedersen-2003-evaluation,och-ney-2003-systematic,Ostling2016efmaral,jalili-sabet-etal-2020-simalign}.

Precision, recall, F-measure and \acrshort{aer} are defined as follows:

\[
	\text{Recall} = \frac{|A\cap S|}{|S|},~~~~\text{Precision}  = \frac{|A\cap P|}{|A|}, F_1 = 2\frac{\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}}
\]

\[
	\text{AER} = 1- \frac{|A\cap S|+|A\cap P|}{|A|+|S|}
\]

With $A$ being the set of alignments generated by the model, $S$ being the set of Sure alignments and $P$ the set of Possible alignments.

For a short discussion on the problems of evaluation, see Section~\ref{sec:problems-evaluation}.




\section{Baseline Systems}
I chose two baseline systems: \texttt{fast\_align} \autocite{dyer-etal-2013-simple} and \texttt{eflomal} \autocite{Ostling2016efmaral}.

\subsection{fast\_align}
\texttt{fast\_align} is a re-parameterization of the IBM Model 2. 
It has become a popular seccessor to Giza++, serves as a baseline system in other works \autocites{Ostling2016efmaral,jalili-sabet-etal-2020-simalign}, and is even recommended by WHO? as an alternative for Giza++ for computing the word alignments for Moses SMT. 
It outperforms Giza++ in many scenarios.

\texttt{fast\_align} is extremely fast---computing the word alignments for the around \numprint{80000} sentence pairs took around 50 seconds. 
It is well documented and is extremely easy to compile and to operate. 
All of this makes \texttt{fast\_align} the most attractive system to use as a baseline system.

\subsection{eflomal}
\texttt{eflomal} is ... 



\subsection{Performance}
To test the baseline systems (\texttt{fast\_align}) I word-aligned all of the sentence pairs (\numprint{79548}), then extracted the alignments for the 600 annotated sentences and again compared my alignments with those produced by \texttt{fast\_align}. 
The results are shown in Table~\ref{tab:baseline}.
\begin{table}
\centering
\begin{tabular}{llccccc}
\toprule
											&Method &Dataset Size & Percision & Recall & $F_1$    & AER \\
\midrule 
\multirow{5}{1em}{\rotatebox{90}{Baseline}}& \multirow{2}{*}{fast\_align} & \numprint{79548}	  & \textbf{0.622}	  & \textbf{0.782}  & \textbf{0.693} & \textbf{0.307} \\
									    	%& " &50k         & 0.62	  & 0.775  & 0.689  & 0.311  \\
									    	&  & 25k         & 0.603	  & 0.754  & 0.67 & 0.33 \\
									    	%& " & 10k   	  & 0.581	  & 0.727  & 0.646 & 0.354 \\
									    	%& " & 5k 		  & 0.564	  & 0.709  & 0.628 & 0.372 \\
									    	%& " & 1k          & 0.529     & 0.664 & 0.589 & 0.411 \\
									    	&  & 600 		  & 0.515	  & 0.644  & 0.572 & 0.427 \\
										 \cmidrule{2-7}
										 & \multirow{2}{*}{eflomal} & \numprint{79548} & 0.827 & 0.877 & 0.851 & 0.148 \\
										 & 		& 600 & 			0.707 & 0.724 &  0.715 & 0.284\\
\bottomrule
\end{tabular}
\caption{Evaluation metrics for word alignments with the baseline model (fast\_align) for different dataset sizes.
\enquote{Dataset Size} refers to the number of sentence pairs. }
\label{tab:baseline}
\end{table}



\section{SimAlign}
I tested SimAlign with different parameters to word align the 600 setence pairs of German-Romansh, for which I created a gold standard (see Chapter~\ref{chap:gold-standard}). 

I tested the two multilingual embeddings that SimAlign works with out-of-the-box: mBERT\footnote{\url{https://github.com/google-research/bert/blob/master/multilingual.md}} and XLM-R\autocite{conneau-etal-2020-xlm}. 
mBERT only works on a subword level (BPE), while XLM-R works either on the word or the subword level. 

For each embedding and word/subword-level combination, alignments are produced according to each of the three methods presented by \cite{jalili-sabet-etal-2020-simalign} (see also Section~\ref{subsec:simalign-method}).

\subsection{Results}
Table~\ref{tab:simalign} shows the evaluation metrics for word alignments computed with SimAlign with the different methods.

To evaluate the alignments against the gold standard, I used a script provided by the creators of SimAlign\footnotemark.
\footnotetext{\url{https://github.com/cisnlp/simalign/blob/master/scripts/calc\_align_score.py}}
 
For each embedding layer (mBERT and XLM-R), the best score for each column is marked in bold. 
Generally, the mBERT embeddings perform better. 
Argmax has the best percision (0.894), which means only 10.6\% of the alignments are wrong. 
However, it has \gls{recall} measure of only 0.622, which means 37.8\% of the alignments are missing.
Match has the lowest percision (0.795) but the highest recall (0.767), which makes it the best compromise between percision and recall and it thus has the lowest \acrshort{aer}.

\begin{table}
\centering
\begin{tabular}{llllcccc}
\toprule
	                                       &	 Embedding	     & Level		              & Method & Percision & Recall & $F_1$     & AER \\
\midrule
\multirow{9}{1em}{\rotatebox{90}{SimAlign}} & \multirow{3}{*}{mBert} & \multirow{3}{*}{BPE}  &  Argmax & \textbf{0.894}    & 0.622	& 0.734  & 0.266 \\
											&							&				     &  Itermax & 0.832  		  & 0.731	& 0.778  & 0.222 \\
											&						  &						 &  Match   & 0.795   		 & \textbf{0.767}  & \textbf{0.781}  & \textbf{0.219} \\	
											\cmidrule{2-8}
											& \multirow{6}{*}{XLM-R} & \multirow{3}{*}{Word} &  Argmax  & \textbf{0.848}	  		 & 0.399  & 0.543  & 0.457 \\
											&						&						 & Itermax  & 0.767  		  & 0.504  & 0.608  & 0.391 \\
											&						&					     & Match    & 0.67   		  & 0.647	& \textbf{0.658}	 & \textbf{0.342} \\
																	\cmidrule{3-8}
											&						& \multirow{3}{*}{BPE}	 &	Argmax  & 0.773   		 & 0.488  & 0.598  & 0.402 \\
											&					    &						 & Itermax  & 0.671  		  & 0.595  & 0.631  & 0.369 \\
											&						&						& Match		& 0.558	 		  & \textbf{0.719}  & 0.628  & 0.372 \\


\bottomrule
\end{tabular}
\caption{Evaluation metrics for word alignments using SimAlign, with different embeddings and word/sub-word level. 
Best result per embedding type in bold.}
\label{tab:simalign}
\end{table}


\section{Discussion}
Comparing the best performance of SimAlign against the best performance of the baseline systems, SimAlign outperforms \texttt{fast\_align}, but is outperformed by \texttt{eflomal}.

Nonetheless, I believe the results are still surprising and promising. 
SimAlign uses embeddings from language models which have never seen Romansh, a scenario which is also referred to as zero-shot. 
Despite this fact, the performance is excellent. 
SimAlign's recall is on par with fast\_align and its precision is 27\% higher than that of fast\_align. 

Also, in the hypothetical case that we only had the 600 annotated sentences to compute word alignment, SimAlign would have outperformed eflomal as well with an \acrshort{aer} of 0.284 (SimAlign) against an \acrshort{aer} of 0.219 (eflomal). 

Further, the SimAlign's performance on the language pair German-Romansh (\acrshort{aer} 0.219) doesn't fall from the performance of SimAlign on English-German (\acrshort{aer} 0.21 (Table 2 in \cite{jalili-sabet-etal-2020-simalign})), which means performance in a zero-shot setting  with mBERT embeddings  for German-Romansh is just as good.

\begin{table}
\centering
\begin{tabular}{lcccc}
	\toprule
							Method & Precision & Recall & $F_1$ & AER \\
\midrule
  fast\_align& 0.622	  & 0.782  & 0.693 & 0.307 \\

							eflomal     & \textbf{0.827} & \textbf{0.877} & \textbf{0.851} & \textbf{0.148} \\

SimAlign:                     mBERT-BPE & 0.795   & 0.767  & 0.781  & 0.219 \\
\bottomrule
\end{tabular}
\caption{Comparison of the best performance of each of the three methods. 
The best value in each column is in bold.}
\label{tab:comparison}
\end{table}


\subsection{General Problems with Evaluation}
\label{sec:problems-evaluation}
It should also be mentioned that each word alignment gold standard has different annotation guidelines and might be more preferable or biased towards one model or the other. 
For instance a gold standard which prefers 1-to-1 alignments will reward a model which generates little or no 1-to-many alignments. 
At the same time, it will penlaize the precision performance of a model that generates 1-to-many alignments, although they might be correct.

Handling Sure and Possible alignments in a different way in each gold standard, might also affect the performance evaluation. 
Not using Possible alignments will lead to a lower value for $|A \cap P|$, which will negatively affect precision and will penalize a model that performs better than expected. 
Labeling many of the alignments as Possible alignments instead of Sure will keep $S$ small and thus lead to favorable recall. 


\subsubsection{Problems with the Gold Standard for German-Romansh}
As already explained in Section ??, the gold standard I created is not perfect (no second annotator, no Possible alignments). 
In my annotation guidelines, I preferred 1-to-1 alignments (see Section ??) and used no Possible label for labeling alignments that might still be correct.
Theoratically, not using Possible alignments may explain fast\_align's low percision. 
In theory, it is possible that fast\_align generates \emph{correct} 1-to-many alignments which I ignored in my annotations. 
In that case, we should solely concentrate on recall, which is not affected by Possible alignments. 
If we were indeed to ignore the other measurements, fast\_align would beat SimAlign by 2\%, which are insignifcant.

All that being said, I believe the excellent performance of eflomal proves that the gold standard is of good quality and is sensible for measuring the performance of word alignment models.

\section{Summary}
I evaluated the performance of the two baseline models (fast\_align and eflomal)  and SimAlign, a similarity based word alignment model, using a gold standard of 600 annotated sentence pairs in German-Romansh, which I created myself.
I compared the performance of two baseline statistical models with the performance of SimAlign using multilingual embeddings in a zero-shot setting. 
SimAlign outperformed fast\_align, but not eflomal (see Table~\ref{tab:comparison}). 

SimAlign's performance, altough worse the eflomal's performance, is promising. 
It shows that mBERT's embeddings may be used in a zero-shot setting for the task of word alignment and may give others the impulse to testing the performance of mBERT (or other multilingual models) on Romansh in other tasks, such as information extraction, question answering, sentiment analysis etc.




