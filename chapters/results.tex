\chapter{Results}\label{chap:results}

After having created a gold standard (see Chapter~\ref{chap:gold-standard}) for evaluating the quality of the alignments, I compared the alignments computed by SimAlign with the alignments computed by two baseline systems.
I shall now proceed to present the results of these experiments.

\section{Evaluation Metrics}
\label{sec:evaluation-metrics}
To evaluate the quality of word alignment, four measures are used. 
The first three---precision, recall and F-measure---are traditional measures in information retrieval \autocite{mihalcea-pedersen-2003-evaluation}.

Precision is the percentage of items that the system retrieved, which are indeed positive.
It answers the question \enquote{how many of the items marked as positive by the system are in fact positive?} and is defined as $\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}}$, with TP being \enquote{true positives} and FP being \enquote{false positives} \autocite[67]{jurafsky-2019}.

Recall is the percentage of true positives retrieved by the system out of all positives.
It answers the question \enquote{how many of all the true positives were actually found by the system?} and is defined as $\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}$, with TP being \enquote{true positives} and FN being \enquote{false negatives} \autocite[67]{jurafsky-2019}.

F-measure is a score that incorporates precision and recall. 
The fourth measurement for evaluating word alignment, \acrfull{aer}, was introduced by \cite{och-ney-2000-improved}. 

For computing the evaluation scores of the word alignments, I used a script made available on GitHub\footnotemark by the creators of SimAlign \autocite{jalili-sabet-etal-2020-simalign}. 
\footnotetext{\url{https://github.com/cisnlp/simalign/blob/master/scripts/calc_align_score.py}}
The script uses a definition of precision, recall and AER which stems from \cite{och-ney-2000-improved} and was later used by many others \autocites{mihalcea-pedersen-2003-evaluation,och-ney-2003-systematic,Ostling2016efmaral,jalili-sabet-etal-2020-simalign}. Precision, recall, F-measure and \acrshort{aer} are defined as follows:

\[
	\text{Recall} = \frac{|A\cap S|}{|S|},~~~~\text{Precision}  = \frac{|A\cap P|}{|A|},~~~F_1 = 2\frac{\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}}
\]

\[
	\text{AER} = 1- \frac{|A\cap S|+|A\cap P|}{|A|+|S|}
\]

With $A$ being the set of alignments generated by the model, $S$ being the set of Sure alignments and $P$ the set of Possible alignments, and $S \subseteq P$, meaning the set of possible alignment $P$ contains also all of the Sure alignments \autocite{och-ney-2000-improved}.

I will later discuss shortly some of the problems I see in these evaluation schemes (Section~\ref{sec:problems-evaluation}).


\section{Baseline Systems}
I chose two baseline systems: fast\_align \autocite{dyer-etal-2013-simple} and eflomal \autocite{Ostling2016efmaral}. 
Both have established themselves as well performing models and were used as baseline models in previous works \autocites{Ostling2016efmaral,jalili-sabet-etal-2020-simalign,steingrimsson-etal-2021-combalign}

\subsection{fast\_align}
fast\_align is a re-parameterization of the IBM Model 2 which overcomes two problems posed by IBM Models 1 and 2: 
IBM Model 1 assumes all word orders are equally likely and Model 2 is \enquote{vastly overparameterized, making it prone to degenerate behavior on account of overfitting.} \autocite{dyer-etal-2013-simple} 

fast\_align overcomes these problems by implementing a log-linear parameterization. It is ten times faster than IBM Model 4 and outperforms it \autocite{dyer-etal-2013-simple}.
It has become a popular competitor to Giza++, serves as a baseline system in other works \autocites{Ostling2016efmaral,jalili-sabet-etal-2020-simalign}, and is even recommended by Philipp Koehn as an alternative to Giza++\footnote{For computing the word alignments for Moses SMT, a software package for training statistical machine translation models}:



\begin{displayquote}
Another alternative to GIZA++ is fast\_align from Dyer et al. It runs much faster, and may even give better results, especially for language pairs without much large-scale reordering. \autocite[115]{koehn-moses-smt-2022}
\end{displayquote}
 

fast\_align is extremely fast---computing the word alignments for the around \numprint{80000} sentence pairs takes around 50 seconds on my system\footnote{MacBook Air (M1, 2020), 8 GB RAM, running macOS Monterey 12.3.1}. 
It is well documented and is extremely easy to compile and to operate. 
All of this makes fast\_align a most attractive system to use as a baseline system.

\subsection{eflomal}
eflomal (a.k.a. efmaral\footnotemark) is a system for word alignment using a Bayesian model with Markov Chain Monte Carlo inference (instead of the usual maximum likelihood estimation used in traditional applications of the IBM models for inference, i.e., updating the probabilities). 
Its performance surpasses fast\_align and is on par with Giza++ \autocite{Ostling2016efmaral}.

\footnotetext{eflomal is a more memory efficient version of efmaral. 
See \url{https://github.com/robertostling/efmaral}}


\subsection{Performance}
Statistical word alignment models rely heavily  on a minimal amount of parallel data before they reach a threshold of good performance. In order to be fair in the evaluation of the baseline systems (fast\_align and eflomal) I word-aligned all of the sentence pairs (\numprint{79548}) with  the addition of the 600 annotated sentences from the gold standard (total of \numprint{80148} sentence pairs). I then extracted the alignments of the gold standard for the evaluation.
 

The performance of the two baseline models on different dataset sizes is presented in Table~\ref{tab:baseline}. The relation between quality and dataset size is striking. 

Compared to  results reported in other papers, the results achieved by the models can be considered good. 
For eflomal,  an \acrshort{aer} of 0.106  was achieved for English-Swedish (\numprint{692662} sentences) and an AER of 0.279 for English-Romanian (\numprint{48641} sentences) (Table 2 in \cite{Ostling2016efmaral}). 
Trained on \numprint{50000} sentence pairs of German-French, Giza achieves an AER of 0.156; trained on \numprint{100000} an AER of 0.125 is achieved (Table 5 in \cite{och-ney-2000-improved}). 
The AER of 0.148 achieved for German-Romansh using eflomnal is within this range.



The results are further discussed in Section~\ref{sec:discussion}.

\begin{table}
\centering
\begin{tabular}{cccccc}
\toprule
											Method &Dataset Size & Precision & Recall & $F_1$    & AER \\
\midrule 
%\multirow{12}{1em}{\rotatebox{90}{Baseline}}& 
\multirow{6}{*}{\rotatebox{90}{fast\_align}} & \numprint{80148}	  & \textbf{0.622}	  & \textbf{0.782}  & \textbf{0.693} & \textbf{0.307} \\
																												    	  &50k         & 0.62	  & 0.775  & 0.689  & 0.311  \\
																												    	  & 25k         & 0.603	  & 0.754  & 0.67 & 0.33 \\
																												    	  & 10k   	  & 0.581	  & 0.727  & 0.646 & 0.354 \\
																												    	  & 5k 		  & 0.564	  & 0.709  & 0.628 & 0.372 \\
																												    	% &  & 1k          & 0.529     & 0.664 & 0.589 & 0.411 \\
																												    	  & 600 		  & 0.515	  & 0.644  & 0.572 & 0.427 \\
										 \cmidrule{1-6}
										  \multirow{6}{*}{\rotatebox{90}{eflomal}} & \numprint{80148} & \textbf{0.827} & \textbf{0.877} & \textbf{0.851} & \textbf{0.148} \\
										 													&						50k		 & 0.828 & 0.86 & 0.844 & 0.156 \\
										 														&						25k		& 0.812  &0.836 & 0.824 & 0.176 \\
										 														&						10k		&	0.798 & 0.805 & 0.801 & 0.199 \\
										 														&						5k    & 0.776 & 0.78 & 0.778 & 0.222\\
										 											 		& 600              & 0.707 & 0.724 &  0.715 & 0.284\\
\bottomrule
\end{tabular}
\caption[Word alignment quality of the baseline models]{Word alignment quality of the baseline models, tested on different dataset sizes. 
Best result per method in bold.
\enquote{Dataset Size} refers to the number of sentence pairs. 
The full dataset size  is the number of sentence pairs extracted at the time of the experiments (\numprint{79548}) plus the 600 annotated sentence pairs from the gold standard.}
\label{tab:baseline}
\end{table}



\section{SimAlign}\label{sec:performance-simalign}
I word-aligned the 600 sentences from the gold standard (see Chapter~\ref{chap:gold-standard}) several times using different parameters. 
I tested the two multilingual embeddings that SimAlign works with out-of-the-box: mBERT\footnote{\url{https://github.com/google-research/bert/blob/master/multilingual.md}} and XLM-R \autocite{conneau-etal-2020-xlm}. 
mBERT only provides embeddings on a subword level (called WordPiece), while XLM-R works either on the word or the subword level (BPE) \autocite{jalili-sabet-etal-2020-simalign} (see also Section~\ref{sec:subwords}). 

For each embedding and word/subword-level combination, alignments are produced according to each of the three methods (Argmax, Itermax and Match) presented by \cite{jalili-sabet-etal-2020-simalign} (see also Section~\ref{subsec:simalign-method}).

\subsection{Performance}
Table~\ref{tab:simalign} and Figure~\ref{fig:aer-comparison} show the evaluation of performance for word alignments computed with SimAlign with the various methods. 
For each embedding layer (mBERT and XLM-R), the best score in each column is marked in bold. 
Generally, the mBERT embeddings perform better. 
Argmax has the best precision (0.894), which means only 10.6\% of the alignments are wrong. 
However, it has recall measure of only 0.622, which means 37.8\% of the alignments are missing.
Match has the lowest precision (0.795) but the highest recall (0.767), which makes it the best compromise between precision and recall and it thus has the lowest \acrshort{aer}.

These results are reasonable and within the range of reported results for other language pairs using SimAlign. 
SimAlign's AER ranges between 0.06 for English-French, and 0.39 for English-Hindi. 
For English-Romanian an AER of 0.29 was achieved, and for English-German an AER of 0.19 (Table 2 in \cite{jalili-sabet-etal-2020-simalign}). 
This puts the minimal AER of 0.19 achieved for German-Romansh in a reasonable place within this range.

\begin{table}
\centering
\begin{tabular}{llllcccc}
\toprule
	                                       &	 Embedding	     & Level		              & Method & Percision & Recall & $F_1$     & AER \\
\midrule
\multirow{9}{1em}{\rotatebox{90}{SimAlign}} & \multirow{3}{*}{mBERT} & \multirow{3}{*}{Subword}  &  Argmax & \textbf{0.894}    & 0.622	& 0.734  & 0.266 \\
											&							&				     &  Itermax & 0.832  		  & 0.731	& 0.778  & 0.222 \\
											&						  &						 &  Match   & 0.795   		 & \textbf{0.767}  & \textbf{0.781}  & \textbf{0.219} \\	
											\cmidrule{2-8}
											& \multirow{6}{*}{XLM-R} & \multirow{3}{*}{Word} &  Argmax  & \textbf{0.848}	  		 & 0.399  & 0.543  & 0.457 \\
											&						&						 & Itermax  & 0.767  		  & 0.504  & 0.608  & 0.391 \\
											&						&					     & Match    & 0.67   		  & 0.647	& \textbf{0.658}	 & \textbf{0.342} \\
																	\cmidrule{3-8}
											&						& \multirow{3}{*}{Subword}	 &	Argmax  & 0.773   		 & 0.488  & 0.598  & 0.402 \\
											&					    &						 & Itermax  & 0.671  		  & 0.595  & 0.631  & 0.369 \\
											&						&						& Match		& 0.558	 		  & \textbf{0.719}  & 0.628  & 0.372 \\


\bottomrule
\end{tabular}
\caption[Word alignment quality using SimAlign]{Word alignment quality using SimAlign, with different embeddings and word/sub-word level. 
Best result per embedding type in bold.}
\label{tab:simalign}
\end{table}

\begin{figure}
\centering
\includegraphics{graphics/charts/aer-comparison.png}
\caption[Comparison of AER for different methods and embeddings using SimAlign]{Comparison of \acrfull{aer} (lower is better) for different methods and embeddings using SimAlign}
\label{fig:aer-comparison}
\end{figure}


\section{Discussion}\label{sec:discussion}
Comparing the best performance of SimAlign against the best performance of the baseline systems, SimAlign outperforms fast\_align, but is outperformed by eflomal.

Nonetheless, I believe that these results are good news. 
SimAlign uses embeddings from language models which have never seen Romansh, a scenario which is also referred to as \enquote{zero-shot}. 
Despite this fact, the performance is excellent. 
SimAlign's recall is on par with that of fast\_align and its precision is higher than that of fast\_align by 17.3 percentage points (27.8\%).
Also, in the hypothetical case in which we only had the 600 annotated sentences to compute word alignment, SimAlign would have outperformed eflomal as well with an \acrshort{aer} of 0.219  (SimAlign) against an \acrshort{aer} of 0.284 (eflomal) (cf.~Table~\ref{tab:baseline}). 

Further,  SimAlign's performance on the language pair German-Romansh (\acrshort{aer} of 0.19) does not fall from the performance of SimAlign on English-German sentence pairs (\acrshort{aer} of 0.19, Table 2 in \cite{jalili-sabet-etal-2020-simalign}). This means that the performance in a zero-shot setting  with mBERT embeddings  for German-Romansh is virtually as good as the performance for a pair of seen languages.

\begin{table}
\centering
\begin{tabular}{lcccc}
	\toprule
							Method & Precision & Recall & $F_1$ & AER \\
\midrule
  fast\_align& 0.622	  & 0.782  & 0.693 & 0.307 \\

							eflomal     & \textbf{0.827} & \textbf{0.877} & \textbf{0.851} & \textbf{0.148} \\

SimAlign:                     mBERT-subword & 0.795   & 0.767  & 0.781  & 0.219 \\
\bottomrule
\end{tabular}
\caption[Comparison of the best performance of the three SimAlign methods]{Comparison of the best performance of each of the three methods. 
The best value in each column is in bold.}
\label{tab:comparison}
\end{table}

\begin{figure}
\centering
\includegraphics{graphics/charts/precision.png}
\caption{Comparing precision between the systems for different dataset sizes.}
\label{fig:precision}
\end{figure}
\begin{figure}
\centering
\includegraphics{graphics/charts/recall.png}
\caption{Comparing recall between the systems for different dataset sizes.}
\label{fig:recall}
\end{figure}

\begin{figure}
\centering
\includegraphics{graphics/charts/aer.png}
\caption{Comparing \acrshort{aer} between the systems for different dataset sizes.}
\label{fig:aer}
\end{figure}

\subsection{General Problems with Evaluation}
\label{sec:problems-evaluation}
It should also be mentioned that each word alignment gold standard has different annotation guidelines and might be more preferable or biased towards one model or the other. 
For instance, a gold standard which prefers 1-to-1 alignments will reward a model which generates little or no 1-to-many alignments. 
At the same time, it will penalize the precision measurement of a model that generates 1-to-many alignments, even though these alignments might be correct.

Handling Sure and Possible alignments in a different way in each gold standard might also affect the performance evaluation. 
Not using Possible alignments will lead to a lower precision value, since it will have lower values for the union of the generated alignments and the possible alignments $|A \cap P|$ (the nominator of the precision measure, see Section~\ref{sec:evaluation-metrics}). This will negatively affect precision and will penalize a model that performs better than expected. 
Labeling many of the alignments as Possible alignments instead of Sure will keep $|S|$ (the denominator of the recall measure) small and thus lead to favorable recall. 


\subsubsection{Problems with the Gold Standard for German-Romansh}
As already explained in Section~\ref{sec:gold-flaws}, the gold standard I created is not perfect (no second annotator, no Possible alignments). 
In my annotation guidelines, I preferred 1-to-1 alignments (see Section~\ref{sec:gold-principles}) and used no Possible label for labeling alignments that might still be correct.
Theoretically, not using Possible alignments may explain fast\_align's low precision. 
In theory, it is possible that fast\_align generates \emph{correct} 1-to-many alignments which I ignored in my annotations. 
In that case, we should solely concentrate on recall, which is not affected by Possible alignments. 
If we were indeed to ignore the other measurements, the difference between fast\_align (recall 0.782) and SimAlign (recall 0.767) would be 1.5 percentage points, a difference of 2\%, in favor of fast\_align.

% All that being said, I believe the excellent performance of eflomal proves that the gold standard is of good quality and is sensible for measuring the performance of word alignment models on German-Romansh.


\section{Explanation Attempt}
Multilingual models such as mBERT show good performance in what is called \enquote{cross-lingual zero-shot transfer}. 
It is a scenario in which a pre-trained model is fine-tuned (training taking place after the initial pre-training) on a task, e.g., \acrshort{pos} tagging, on one language; the model then carries out this task on a different language (target language) for which it wasn't trained \autocite{deshpande-etal-2022-bert}.
Such models also perform well in a variety of tasks such as \acrshort{pos} tagging or \acrshort{ner} on \textbf{unseen languages} (languages which were not covered by the pre-trained model) such as Faroese, Maltese or Swiss German \autocite{muller-et-al-2020}.

There is a lack of consensus as to what properties of a language favor performance in such scenarios, i.e., it is not entirely clear  \emph{when} zero-shot transfer works. 
Some suggest sub-word overlap is crucial for good performance \autocite{wu-dredze-2019-beto}, while others show that transfer also works well between languages written in different scripts when they are typologically similar\footnotemark{}, meaning sub-word overlap is not a necessary condition \autocite{pires-etal-2019-multilingual}.  It was, however, shown that transliterating languages from unseen scripts leads to large gains in performance \autocite{muller-et-al-2020}.

\footnotetext{mBERT fine-tuned for \acrshort{pos} tagging in Urdu (Arabic script) achieved 91\% accuracy on Hindi (Devanagari script) \autocite{pires-etal-2019-multilingual}. 
Both languages are mutually intelligible and are considered variants of a single language---Hindustani \autocite{hindustani}.}

\cite{deshpande-etal-2022-bert} show that zero-shot transfer is possible for different scripts with similar word order, and that the lack of both, on the other hand, hurts performance. 

\cite{deshpande-etal-2022-bert} also show that zero-shot performance is correlated with alignment between word embeddings, i.e., to what extent the embeddings of different languages share the same geometric shape and are aligned across the same axes: 
When multilingual word embeddings are learned, the embeddings of the different languages have to be aligned to each other, such that they share similar geometrical shapes and are aligned across the same axes, in order for vectors of similar words across different languages to be next to each other in the vector space \autocite[220-223]{koehn-2020}. See Figure~\ref{fig:embedding-alignment}.


\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{graphics/embedding-alignment.png}
\caption[Matching up the geometric shape of embedding spaces of words in English and German]{Matching up the geometric shape of embedding spaces of words in English and German. 
Taken from \cite[223]{koehn-2020}}.
\label{fig:embedding-alignment}
\end{figure}

However, in our case, we are not dealing with transfer learning, but simply with the leverage of embeddings for measuring word similarity. 

Since multilingual models process tokens at the sub-word level, they work in an open vocabulary setting and can process any language, even languages that aren't part of the pretraining data (providing the character set is part of the pretraining data) \autocite{muller-et-al-2020}.

According to the mBERT's performance on unseen languages, \cite{muller-et-al-2020} put these unseen languages into three categories: Easy, Intermediate and Hard. 
\cite{muller-et-al-2020} ascribe the differences in mBERT's performance on these languages to two things: close relatedness to languages used during pretraining; and the unseen languages using the same script as those closely related languages which were seen during pretraining.

Since Romansh shares a high similarity, not only in script, but also typologically, with other Romance, as well as other European languages\footnotemark{}, which are a major part of the training data for mBERT, it should not be surprising that similarity-based word alignment using word embeddings from mBERT works well.

\footnotetext{European languages from different languages families (Germanic, Romance, Slavic) were shown to display high similarity to each other and to form a so-called \emph{Sprachbund}, dubbed Standard Average European \autocite{haspelmath-2001-standard}.}




 
\section{Summary}
I evaluated the performance of  two statistical baseline models (fast\_align and eflomal)  as well as the performance of SimAlign, a similarity-based word alignment model, on the language pair German-Romansh. 
SimAlign computed the word similarity using multilingual word embeddings from two language models: mBERT and XLM-R. Neither of the models had seen Romansh during training, i.e., we are dealing with a zero-shot setting.
The evaluation was done using a gold standard of 600 annotated sentence pairs in German-Romansh, which I had created myself (see Chapter~\ref{chap:gold-standard}).
SimAlign outperformed fast\_align, but not eflomal (see Table~\ref{tab:comparison}). 

SimAlign's performance, although worse the eflomal's performance, is on par with that of fast\_align and is generally promising. 
It shows that mBERT's embeddings can be used in a zero-shot setting (Romansh was not part of the training data; mBERT has never seen Romansh before) for the task of word alignment and may give future students and/or researchers the impulse to test the performance of mBERT (or other multilingual models) on Romansh in other tasks, such as information extraction, question answering, sentiment analysis, \acrshort{pos} tagging etc.

For a discussion of the differences between the systems in some specific cases, see Appendix~\ref{appendix-a}.



